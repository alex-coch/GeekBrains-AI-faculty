{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Тренировка модели \n",
        "Пришло время создать свою собственную модель, со своими словами. Процес тренировки займет несколько часов. После тренировки вы сможете проверить, насколько точной получилась модель."
      ],
      "metadata": {
        "id": "CmEVSo_jXW_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Установка"
      ],
      "metadata": {
        "id": "UQC_qdm5YZmL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Импорт необходимых библиотек"
      ],
      "metadata": {
        "id": "vXDd_nW-Ypvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "!wget https://github.com/tensorflow/tensorflow/archive/v2.4.1.zip\n",
        "!unzip v2.4.1.zip &> 0\n",
        "!mv tensorflow-2.4.1/ tensorflow/\n",
        "import sys\n",
        "sys.path.append(\"/content/tensorflow/tensorflow/examples/speech_commands/\")\n",
        "import input_data\n",
        "import models\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "metadata": {
        "id": "lxYEfnY7Yzxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Конфигурация модели!\n",
        "Далее необходимо выбрать параметры, на которых будет основана наша модель. Для начала выберем, какие слова мы будем тренировать. От этого зависит, многое, так как невыбранные слова модель пометит как \"unknown\", и будет так же на них тренироваться. Можно спокойно выбрать 2-4 слова для тренировки модели.\n",
        "\n",
        "Слова для тренировки(ВЫБИРАЙТЕ ИЗ ЭТОГО СПИСКА): \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\", “backward”, “forward”, “follow”, “learn”.\n",
        "\n",
        "Слова для тренировки, которые помогут нашей модели быть точнее: \"bed\", \"bird\", \"cat\", \"dog\", \"happy\", \"house\", \"marvin\", \"sheila\", \"tree\", \"wow\"."
      ],
      "metadata": {
        "id": "pO0q5snPY-a4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SXbi_6RXVyj"
      },
      "outputs": [],
      "source": [
        "WANTED_WORDS = # ваш код здесь #"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Количество шагов обучения и скорость обучения можно указать через запятую, чтобы определить эти параметры для каждого этапа. Например, TRAINING_STEPS=\"12000,3000\" и LEARNING_RATE=\"0.001,0.0001\" запустят 12000 шагов обучения со скоростью 0.001 и 3000 со скоростью 0.0001. Это хорошие настройки, но вы можете попробовать что-то свое!\n",
        "\n",
        "Лучше всего ставить меньбшее количество эпох, и шаг обучения больше."
      ],
      "metadata": {
        "id": "X09paLh3afoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_STEPS = # ваш код здесь #\n",
        "LEARNING_RATE = # ваш код здесь #"
      ],
      "metadata": {
        "id": "qtK7fTbsafFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы определяем именно такую архитектуру нашей модели. Но вы можете попробовать нечто иное, то что мы не рассматривали - single_fc, conv, low_latency_conv, low_latency_svdf, tiny_embedding_conv"
      ],
      "metadata": {
        "id": "Ngn0u47rb5Nh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ARCHITECTURE = 'tiny_conv'"
      ],
      "metadata": {
        "id": "8wbFjoV1b4ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вычисляем конечное количество шагов, для того, чтобы определить контрольные точки\n",
        "\n",
        "TOTAL_STEPS = str(sum(map(lambda string: int(string), TRAINING_STEPS.split(\",\"))))\n",
        "\n",
        "# Выводим общую информацию\n",
        "print(\"Training these words: %s\" % WANTED_WORDS)\n",
        "print(\"Training steps in each stage: %s\" % TRAINING_STEPS)\n",
        "print(\"Learning rate in each stage: %s\" % LEARNING_RATE)\n",
        "print(\"Total number of training steps: %s\" % TOTAL_STEPS)"
      ],
      "metadata": {
        "id": "6uKrfEIHcLSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Вычисляем процентаж слов unknown и silence\n",
        "\n",
        "number_of_labels = WANTED_WORDS.count(',') + 1\n",
        "number_of_total_labels = number_of_labels + 2 \n",
        "equal_percentage_of_training_samples = int(100.0/(number_of_total_labels))\n",
        "SILENT_PERCENTAGE = equal_percentage_of_training_samples\n",
        "UNKNOWN_PERCENTAGE = equal_percentage_of_training_samples\n",
        "\n",
        "# Константы, которые используются во время обучения и вывода\n",
        "PREPROCESS = 'micro'\n",
        "WINDOW_STRIDE = 20\n",
        "\n",
        "# Константы, которые используются только в обучении\n",
        "VERBOSITY = 'DEBUG'\n",
        "EVAL_STEP_INTERVAL = '1000'\n",
        "SAVE_STEP_INTERVAL = '1000'\n",
        "\n",
        "# Константы для путей к файлам\n",
        "DATASET_DIR =  'dataset/'\n",
        "LOGS_DIR = 'logs/'\n",
        "TRAIN_DIR = 'train/' # для контрольных точек\n",
        "\n",
        "\n",
        "import os\n",
        "MODELS_DIR = 'models'\n",
        "if not os.path.exists(MODELS_DIR):\n",
        "  os.mkdir(MODELS_DIR)\n",
        "MODEL_TF = os.path.join(MODELS_DIR, 'model.pb')\n",
        "MODEL_TFLITE = os.path.join(MODELS_DIR, 'model.tflite')\n",
        "FLOAT_MODEL_TFLITE = os.path.join(MODELS_DIR, 'float_model.tflite')\n",
        "MODEL_TFLITE_MICRO = os.path.join(MODELS_DIR, 'model.cc')\n",
        "SAVED_MODEL = os.path.join(MODELS_DIR, 'saved_model')\n",
        "\n",
        "# Константы для квантования\n",
        "QUANT_INPUT_MIN = 0.0\n",
        "QUANT_INPUT_MAX = 26.0\n",
        "QUANT_INPUT_RANGE = QUANT_INPUT_MAX - QUANT_INPUT_MIN\n",
        "\n",
        "# Константы для обработки звука\n",
        "SAMPLE_RATE = 16000\n",
        "CLIP_DURATION_MS = 1000\n",
        "WINDOW_SIZE_MS = 30.0\n",
        "FEATURE_BIN_COUNT = 40\n",
        "BACKGROUND_FREQUENCY = 0.8\n",
        "BACKGROUND_VOLUME_RANGE = 0.1\n",
        "TIME_SHIFT_MS = 100.0\n",
        "\n",
        "# Адрес для загрузки данных\n",
        "DATA_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz'\n",
        "VALIDATION_PERCENTAGE = 10\n",
        "TESTING_PERCENTAGE = 10"
      ],
      "metadata": {
        "id": "HHgNl0s0cglk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def window_counter(total_samples, window_size, stride):\n",
        "  '''helper function to count the number of full-length overlapping windows'''\n",
        "  window_count = 0\n",
        "  sample_index = 0\n",
        "  while True:\n",
        "    window = range(sample_index,sample_index+stride)\n",
        "    if window.stop < total_samples:\n",
        "      window_count += 1\n",
        "    else:\n",
        "      break\n",
        "    \n",
        "    sample_index += stride\n",
        "  return window_count\n",
        "\n",
        "OVERLAPPING_WINDOWS = window_counter(CLIP_DURATION_MS, int(WINDOW_SIZE_MS), WINDOW_STRIDE)\n",
        "FLATTENED_SPECTROGRAM_SHAPE = (1, OVERLAPPING_WINDOWS * FEATURE_BIN_COUNT)"
      ],
      "metadata": {
        "id": "UrCsfNqxd0R5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тренировка модели"
      ],
      "metadata": {
        "id": "oIumBb5eeAN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorBoard\n",
        "\n",
        "Для визуализации тренировки процесса, мы добавляем TensorBoard. Иногда может не отображать данные. Перезапускайте если что."
      ],
      "metadata": {
        "id": "y8gXhz8feFg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {LOGS_DIR}"
      ],
      "metadata": {
        "id": "fMvEhkoze3Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Запуск обучения \n",
        "\n",
        "Для обучения глубоких нейронных сетей можно использовать как CPU - вычислительные мощности центрального процессора, так и GPU - графический процессор. Центральный процессор в среднем имеет десять ядер, и может совершать сложные вычисления, но их параллельное количество невелико. С GPU дела обстоят иначе. Графические процессоры несут в себе тысячи ядер, но каждое из них работает на гораздо меньшей тактовой частоте, чем в CPU. В общем и целом, графические ядра прекрасно подходят для задач машинного обучения, особенно для задачи классификации, так как там задачи можно разбить на множество паралллельных задач, и делать их одновременно. \n",
        "\n",
        "Мы можем переключить тренировку модели на GPU, и это существенно сократит время обучения. Для этого нужно зайти в Среда выполения - Сменить среду выполнения. И выбрать GPU."
      ],
      "metadata": {
        "id": "ZqLmRvC0W8e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python tensorflow/tensorflow/examples/speech_commands/train.py \\\n",
        "--data_dir={DATASET_DIR} \\\n",
        "--wanted_words={WANTED_WORDS} \\\n",
        "--silence_percentage={SILENT_PERCENTAGE} \\\n",
        "--unknown_percentage={UNKNOWN_PERCENTAGE} \\\n",
        "--preprocess={PREPROCESS} \\\n",
        "--window_stride={WINDOW_STRIDE} \\\n",
        "--model_architecture={MODEL_ARCHITECTURE} \\\n",
        "--how_many_training_steps={TRAINING_STEPS} \\\n",
        "--learning_rate={LEARNING_RATE} \\\n",
        "--train_dir={TRAIN_DIR} \\\n",
        "--summaries_dir={LOGS_DIR} \\\n",
        "--verbosity={VERBOSITY} \\\n",
        "--eval_step_interval={EVAL_STEP_INTERVAL} \\\n",
        "--save_step_interval={SAVE_STEP_INTERVAL}"
      ],
      "metadata": {
        "id": "fqAfjRhHb2dP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Создание модели\n",
        "\n",
        "Теперь, как в примере с конвертером, мы возьмем последнюю контрольную точку, и преобразуем ее в TF Lite."
      ],
      "metadata": {
        "id": "fE7qBGELcCa7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создание модели для вывода(замораживаем)"
      ],
      "metadata": {
        "id": "P-OciKUOchDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf {SAVED_MODEL}\n",
        "!python tensorflow/tensorflow/examples/speech_commands/freeze.py \\\n",
        "--wanted_words=$WANTED_WORDS \\\n",
        "--window_stride_ms=$WINDOW_STRIDE \\\n",
        "--preprocess=$PREPROCESS \\\n",
        "--model_architecture=$MODEL_ARCHITECTURE \\\n",
        "--start_checkpoint=$TRAIN_DIR$MODEL_ARCHITECTURE'.ckpt-'{TOTAL_STEPS} \\\n",
        "--save_format=saved_model \\\n",
        "--output_file={SAVED_MODEL}"
      ],
      "metadata": {
        "id": "EdPiv5-Hcx_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создание модели TF Lite\n"
      ],
      "metadata": {
        "id": "iIj39DZNc6OU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_settings = models.prepare_model_settings(\n",
        "    len(input_data.prepare_words_list(WANTED_WORDS.split(','))),\n",
        "    SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
        "    WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
        "audio_processor = input_data.AudioProcessor(\n",
        "    DATA_URL, DATASET_DIR,\n",
        "    SILENT_PERCENTAGE, UNKNOWN_PERCENTAGE,\n",
        "    WANTED_WORDS.split(','), VALIDATION_PERCENTAGE,\n",
        "    TESTING_PERCENTAGE, model_settings, LOGS_DIR)"
      ],
      "metadata": {
        "id": "P2sQa8chdCP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "  float_converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
        "  float_tflite_model = float_converter.convert()\n",
        "  float_tflite_model_size = open(FLOAT_MODEL_TFLITE, \"wb\").write(float_tflite_model)\n",
        "  print(\"Float model is %d bytes\" % float_tflite_model_size)\n",
        "\n",
        "  converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL)\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  converter.inference_input_type = tf.lite.constants.INT8  \n",
        "  converter.inference_output_type = tf.lite.constants.INT8\n",
        "  def representative_dataset_gen():\n",
        "    for i in range(100):\n",
        "      data, _ = audio_processor.get_data(1, i*1, model_settings,\n",
        "                                         BACKGROUND_FREQUENCY, \n",
        "                                         BACKGROUND_VOLUME_RANGE,\n",
        "                                         TIME_SHIFT_MS,\n",
        "                                         'testing',\n",
        "                                         sess)\n",
        "      flattened_data = np.array(data.flatten(), dtype=np.float32).reshape(FLATTENED_SPECTROGRAM_SHAPE)\n",
        "      yield [flattened_data]\n",
        "  converter.representative_dataset = representative_dataset_gen\n",
        "  tflite_model = converter.convert()\n",
        "  tflite_model_size = open(MODEL_TFLITE, \"wb\").write(tflite_model)\n",
        "  print(\"Quantized model is %d bytes\" % tflite_model_size)"
      ],
      "metadata": {
        "id": "i62YV1XXdHs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Проверяем точность модели после квантования"
      ],
      "metadata": {
        "id": "9_az6lq3dPgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_tflite_inference_testSet(tflite_model_path, model_type=\"Float\"):\n",
        "  #\n",
        "  # \n",
        "  #\n",
        "  np.random.seed(0) \n",
        "  with tf.Session() as sess:\n",
        "    test_data, test_labels = audio_processor.get_data(\n",
        "        -1, 0, model_settings, BACKGROUND_FREQUENCY, BACKGROUND_VOLUME_RANGE,\n",
        "        TIME_SHIFT_MS, 'testing', sess)\n",
        "  test_data = np.expand_dims(test_data, axis=1).astype(np.float32)\n",
        "\n",
        "  #\n",
        "  # \n",
        "  #\n",
        "  interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "  interpreter.allocate_tensors()\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "  \n",
        "  #\n",
        "  # \n",
        "  #\n",
        "  if model_type == \"Quantized\":\n",
        "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "    test_data = test_data / input_scale + input_zero_point\n",
        "    test_data = test_data.astype(input_details[\"dtype\"])\n",
        "\n",
        "  #\n",
        "  # \n",
        "  #\n",
        "  correct_predictions = 0\n",
        "  for i in range(len(test_data)):\n",
        "    interpreter.set_tensor(input_details[\"index\"], test_data[i])\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "    top_prediction = output.argmax()\n",
        "    correct_predictions += (top_prediction == test_labels[i])\n",
        "\n",
        "  print('%s model accuracy is %f%% (Number of test samples=%d)' % (\n",
        "      model_type, (correct_predictions * 100) / len(test_data), len(test_data)))"
      ],
      "metadata": {
        "id": "_e7O5oZodYNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Точность модели, где были числа с плавающей запятой\n",
        "run_tflite_inference_testSet(FLOAT_MODEL_TFLITE)\n",
        "\n",
        "# Точность модели после квантования\n",
        "run_tflite_inference_testSet(MODEL_TFLITE, model_type='Quantized')"
      ],
      "metadata": {
        "id": "OTS2qS7hdlog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Тестируем модель со своими данными!"
      ],
      "metadata": {
        "id": "G1CufC-Xdzoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ffmpeg-python &> 0\n",
        "from IPython.display import HTML, Audio\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read as wav_read\n",
        "import io\n",
        "import ffmpeg\n",
        "!pip install librosa\n",
        "import librosa\n",
        "import scipy.io.wavfile\n",
        "!git clone https://github.com/petewarden/extract_loudest_section.git\n",
        "!make -C extract_loudest_section/\n",
        "print(\"Packages Imported, Extract_Loudest_Section Built\")"
      ],
      "metadata": {
        "id": "r5ITbxZod3Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Определяем функцию помощника для запуска модели"
      ],
      "metadata": {
        "id": "rtcsVGXVeBUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функия помощник запускает модель с одним входным словом\n",
        "TF_SESS = tf.compat.v1.InteractiveSession()\n",
        "def run_tflite_inference_singleFile(tflite_model_path, custom_audio, sr_custom_audio, model_type=\"Float\"):\n",
        "  #\n",
        "  # \n",
        "  #\n",
        "  # \n",
        "  custom_audio_resampled = librosa.resample(librosa.to_mono(np.float64(custom_audio)), sr_custom_audio, SAMPLE_RATE)\n",
        "  # \n",
        "  scipy.io.wavfile.write('custom_audio.wav', SAMPLE_RATE, np.int16(custom_audio_resampled))\n",
        "  !/tmp/extract_loudest_section/gen/bin/extract_loudest_section custom_audio.wav ./trimmed\n",
        "\n",
        "  custom_model_settings = models.prepare_model_settings(\n",
        "      0, SAMPLE_RATE, CLIP_DURATION_MS, WINDOW_SIZE_MS,\n",
        "      WINDOW_STRIDE, FEATURE_BIN_COUNT, PREPROCESS)\n",
        "  custom_audio_processor = input_data.AudioProcessor(None, None, 0, 0, '', 0, 0,\n",
        "                                                    model_settings, None)\n",
        "  custom_audio_preprocessed = custom_audio_processor.get_features_for_wav(\n",
        "                                        'trimmed/custom_audio.wav', model_settings, TF_SESS)\n",
        "  custom_audio_input = custom_audio_preprocessed[0].flatten()\n",
        "  test_data = np.reshape(custom_audio_input,(1,len(custom_audio_input)))\n",
        "\n",
        "  #\n",
        "  # \n",
        "  #\n",
        "  interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "  interpreter.allocate_tensors()\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  #\n",
        "  # \n",
        "  #\n",
        "  if model_type == \"Quantized\":\n",
        "    input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "    test_data = test_data / input_scale + input_zero_point\n",
        "    test_data = test_data.astype(input_details[\"dtype\"])\n",
        "\n",
        "  #\n",
        "  # \n",
        "  #\n",
        "  interpreter.set_tensor(input_details[\"index\"], test_data)\n",
        "  interpreter.invoke()\n",
        "  output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "  top_prediction = output.argmax()\n",
        "\n",
        "  #\n",
        "  # \n",
        "  #\n",
        "  top_prediction_str = ''\n",
        "  if top_prediction >= 2:\n",
        "    top_prediction_str = WANTED_WORDS.split(',')[top_prediction-2]\n",
        "  elif top_prediction == 0:\n",
        "    top_prediction_str = 'silence'\n",
        "  else:\n",
        "    top_prediction_str = 'unknown'\n",
        "\n",
        "  print('%s model guessed the value to be %s' % (model_type, top_prediction_str))"
      ],
      "metadata": {
        "id": "DhtRmAnweGFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вот это поможет считать ваш голос с микрофона ноутбука/компьютера"
      ],
      "metadata": {
        "id": "MWKb9h66ekhV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_HTML = \"\"\"\n",
        "<script>\n",
        "var my_div = document.createElement(\"DIV\");\n",
        "var my_p = document.createElement(\"P\");\n",
        "var my_btn = document.createElement(\"BUTTON\");\n",
        "var t = document.createTextNode(\"Press to start recording\");\n",
        "\n",
        "my_btn.appendChild(t);\n",
        "//my_p.appendChild(my_btn);\n",
        "my_div.appendChild(my_btn);\n",
        "document.body.appendChild(my_div);\n",
        "\n",
        "var base64data = 0;\n",
        "var reader;\n",
        "var recorder, gumStream;\n",
        "var recordButton = my_btn;\n",
        "\n",
        "var handleSuccess = function(stream) {\n",
        "  gumStream = stream;\n",
        "  var options = {\n",
        "    //bitsPerSecond: 8000, //chrome seems to ignore, always 48k\n",
        "    mimeType : 'audio/webm;codecs=opus'\n",
        "    //mimeType : 'audio/webm;codecs=pcm'\n",
        "  };            \n",
        "  //recorder = new MediaRecorder(stream, options);\n",
        "  recorder = new MediaRecorder(stream);\n",
        "  recorder.ondataavailable = function(e) {            \n",
        "    var url = URL.createObjectURL(e.data);\n",
        "    var preview = document.createElement('audio');\n",
        "    preview.controls = true;\n",
        "    preview.src = url;\n",
        "    document.body.appendChild(preview);\n",
        "\n",
        "    reader = new FileReader();\n",
        "    reader.readAsDataURL(e.data); \n",
        "    reader.onloadend = function() {\n",
        "      base64data = reader.result;\n",
        "      //console.log(\"Inside FileReader:\" + base64data);\n",
        "    }\n",
        "  };\n",
        "  recorder.start();\n",
        "  };\n",
        "\n",
        "recordButton.innerText = \"Recording... press to stop\";\n",
        "\n",
        "navigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\n",
        "\n",
        "\n",
        "function toggleRecording() {\n",
        "  if (recorder && recorder.state == \"recording\") {\n",
        "      recorder.stop();\n",
        "      gumStream.getAudioTracks()[0].stop();\n",
        "      recordButton.innerText = \"Saving the recording... pls wait!\"\n",
        "  }\n",
        "}\n",
        "\n",
        "// https://stackoverflow.com/a/951057\n",
        "function sleep(ms) {\n",
        "  return new Promise(resolve => setTimeout(resolve, ms));\n",
        "}\n",
        "\n",
        "var data = new Promise(resolve=>{\n",
        "//recordButton.addEventListener(\"click\", toggleRecording);\n",
        "recordButton.onclick = ()=>{\n",
        "toggleRecording()\n",
        "\n",
        "sleep(2000).then(() => {\n",
        "  // wait 2000ms for the data to be available...\n",
        "  // ideally this should use something like await...\n",
        "  //console.log(\"Inside data:\" + base64data)\n",
        "  resolve(base64data.toString())\n",
        "\n",
        "});\n",
        "\n",
        "}\n",
        "});\n",
        "      \n",
        "</script>\n",
        "\"\"\"\n",
        "\n",
        "def get_audio():\n",
        "  display(HTML(AUDIO_HTML))\n",
        "  data = eval_js(\"data\")\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  \n",
        "  process = (ffmpeg\n",
        "    .input('pipe:0')\n",
        "    .output('pipe:1', format='wav', ac='1')\n",
        "    .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n",
        "  )\n",
        "  output, err = process.communicate(input=binary)\n",
        "  \n",
        "  riff_chunk_size = len(output) - 8\n",
        "  q = riff_chunk_size\n",
        "  b = []\n",
        "  for i in range(4):\n",
        "      q, r = divmod(q, 256)\n",
        "      b.append(r)\n",
        "\n",
        "\n",
        "  riff = output[:4] + bytes(b) + output[8:]\n",
        "\n",
        "  sr, audio = wav_read(io.BytesIO(riff))\n",
        "\n",
        "  return audio, sr\n",
        "print(\"Chrome Audio Recorder Defined\")"
      ],
      "metadata": {
        "id": "HFJFUPwTehY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Записываем свое аудио и проверяем модель!"
      ],
      "metadata": {
        "id": "fRWbiTrUezyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_audio, sr_custom_audio = get_audio()\n",
        "print(\"DONE\")"
      ],
      "metadata": {
        "id": "QI930Deme6W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "run_tflite_inference_singleFile(MODEL_TFLITE, custom_audio, sr_custom_audio, model_type=\"Quantized\")"
      ],
      "metadata": {
        "id": "MpQEqdV_e9Nj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}