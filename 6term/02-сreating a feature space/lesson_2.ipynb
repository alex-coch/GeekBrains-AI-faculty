{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Создание признакового пространства "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритмы машинного обучения не могут напрямую работать с сырым текстом, поэтому необходимо конвертировать текст в наборы цифр (векторы). Это называется извлечением признаков.\n",
    "\n",
    "##### Мешок слов\n",
    "– это популярная и простая техника извлечения признаков, используемая при работе с текстом. Она описывает вхождения каждого слова в текст.\n",
    "\n",
    "Чтобы использовать модель, нам нужно:\n",
    "\n",
    "- Определить словарь известных слов (токенов).\n",
    "- Выбрать степень присутствия известных слов.\n",
    "\n",
    "Любая информация о порядке или структуре слов игнорируется. Вот почему это называется МЕШКОМ слов. Эта модель пытается понять, встречается ли знакомое слово в документе, но не знает, где именно оно встречается.\n",
    "\n",
    "Интуиция подсказывает, что схожие документы имеют схожее содержимое. Также, благодаря содержимому, мы можем узнать кое-что о смысле документа.\n",
    "\n",
    "Пример:\n",
    "Рассмотрим шаги создания этой модели. Мы используем только 4 предложения, чтобы понять, как работает модель. В реальной жизни вы столкнетесь с бОльшими объемами данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"I like this movie, it's it's it's funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определяем словарь и создаем векторы документа. Соберем все уникальные слова из 4 загруженных предложений, игнорируя регистр, пунктуацию и односимвольные токены. Это и будет наш словарь (известные слова).\n",
    "\n",
    "Для создания словаря можно использовать класс CountVectorizer из библиотеки sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I like this movie, it's it's it's funny.\",\n",
       " 'I hate this movie.',\n",
       " 'This was awesome! I like it.',\n",
       " 'Nice one. I love it.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>aw</th>\n",
       "      <th>f</th>\n",
       "      <th>fu</th>\n",
       "      <th>h</th>\n",
       "      <th>ha</th>\n",
       "      <th>i</th>\n",
       "      <th>i</th>\n",
       "      <th>it</th>\n",
       "      <th>l</th>\n",
       "      <th>...</th>\n",
       "      <th>unn</th>\n",
       "      <th>ve</th>\n",
       "      <th>ve</th>\n",
       "      <th>vi</th>\n",
       "      <th>vie</th>\n",
       "      <th>wa</th>\n",
       "      <th>was</th>\n",
       "      <th>we</th>\n",
       "      <th>wes</th>\n",
       "      <th>y.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    a   aw   f   fu   h   ha   i   i    it   l  ...  unn  ve  ve   vi  vie  \\\n",
       "0   0    0   1    1   0    0   1    0    1   1  ...    1   0    0   1    1   \n",
       "1   0    0   0    0   1    1   0    0    0   0  ...    0   0    0   1    1   \n",
       "2   1    1   0    0   0    0   1    1    1   1  ...    0   0    0   0    0   \n",
       "3   0    0   0    0   0    0   1    1    1   1  ...    0   1    1   0    0   \n",
       "\n",
       "   wa  was  we  wes  y.  \n",
       "0   0    0   0    0   1  \n",
       "1   0    0   0    0   0  \n",
       "2   1    1   1    1   0  \n",
       "3   0    0   0    0   0  \n",
       "\n",
       "[4 rows x 120 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "count_vectorizer = CountVectorizer(ngram_range=(2, 3), analyzer='char', binary=True, tokenizer=str.split)\n",
    "\n",
    "# Создаем the Bag-of-Words модель\n",
    "bag_of_words = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Отобразим Bag-of-Words модель как DataFrame\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "pd.DataFrame(bag_of_words.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда размер словаря увеличивается, вектор документа тоже растет. В примере выше, длина вектора равна количеству известных слов.\n",
    "\n",
    "В некоторых случаях, у нас может быть неимоверно большой объем данных и тогда вектор может состоять из тысяч или миллионов элементов. Более того, каждый документ может содержать лишь малую часть слов из словаря.\n",
    "\n",
    "Как следствие, в векторном представлении будет много нулей. Векторы с большим количеством нулей называются разреженным векторами (sparse vectors), они требуют больше памяти и вычислительных ресурсов. Частично эту проблему можно реить хорошей предобработкой.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Мешок N-грамм\n",
    "\n",
    "Другой, более сложный способ создания словаря – использовать сгруппированные слова. Это изменит размер словаря и даст мешку слов больше деталей о документе. Такой подход называется «N-грамма».\n",
    "\n",
    "N-грамма это последовательность каких-либо сущностей (слов, букв, чисел, цифр и т.д.). В контексте языковых корпусов, под N-граммой обычно понимают последовательность слов. Юниграмма это одно слово, биграмма это последовательность двух слов, триграмма – три слова и так далее. Цифра N обозначает, сколько сгруппированных слов входит в N-грамму. В модель попадают не все возможные N-граммы, а только те, что фигурируют в корпусе.\n",
    "\n",
    "Пример:\n",
    "Рассмотрим такое предложение:The office building is open today\n",
    "\n",
    "Вот его биграммы:\n",
    "\n",
    "- the office\n",
    "- office building\n",
    "- building is\n",
    "- is open\n",
    "- open today\n",
    "\n",
    "Как видно, мешок биграмм – это более действенный подход, чем мешок слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-fc826ec78f94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "text = \"I like this movie, it's funny. I hate this movie. This was awesome! I like it. Nice one. I love it.\"\n",
    "tokenized = text.split()\n",
    "bigrams = ngrams(tokenized, (3))\n",
    "list(bigrams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF\n",
    "\n",
    "У частотного скоринга есть проблема: слова с наибольшей частотностью имеют, соответственно, наибольшую оценку. В этих словах может быть не так много информационного выигрыша для модели, как в менее частых словах. Один из способов исправить ситуацию – понижать оценку слова, которое часто встречается во всех схожих документах. Это называется TF-IDF.\n",
    "\n",
    "TF-IDF (сокращение от term frequency — inverse document frequency) – это статистическая мера для оценки важности слова в документе, который является частью коллекции или корпуса.\n",
    "\n",
    "Скоринг по TF-IDF растет пропорционально частоте появления слова в документе, но это компенсируется количеством документов, содержащих это слово."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='image/tf_idf.PNG'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>awesome</th>\n",
       "      <th>funny</th>\n",
       "      <th>hate</th>\n",
       "      <th>it</th>\n",
       "      <th>like</th>\n",
       "      <th>love</th>\n",
       "      <th>movie</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>this</th>\n",
       "      <th>was</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.812578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259329</td>\n",
       "      <td>0.320323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320323</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259329</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.702035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.539445</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.425305</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.344321</td>\n",
       "      <td>0.539445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.541736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    awesome     funny      hate        it      like      love     movie  \\\n",
       "0  0.000000  0.812578  0.000000  0.259329  0.320323  0.000000  0.320323   \n",
       "1  0.000000  0.000000  0.702035  0.000000  0.000000  0.000000  0.553492   \n",
       "2  0.539445  0.000000  0.000000  0.344321  0.425305  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  0.345783  0.000000  0.541736  0.000000   \n",
       "\n",
       "       nice       one      this       was  \n",
       "0  0.000000  0.000000  0.259329  0.000000  \n",
       "1  0.000000  0.000000  0.448100  0.000000  \n",
       "2  0.000000  0.000000  0.344321  0.539445  \n",
       "3  0.541736  0.541736  0.000000  0.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "document = [\"I like this movie, it's funny funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "values = tfidf_vectorizer.fit_transform(document)\n",
    "\n",
    "# Show the Model as a pandas DataFrame\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "pd.DataFrame(values.toarray(), columns = feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.91629073, 1.91629073, 1.91629073, 1.22314355, 1.51082562,\n",
       "       1.91629073, 1.51082562, 1.91629073, 1.91629073, 1.22314355,\n",
       "       1.91629073])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчеты и частоты могут быть очень полезны, но одним из ограничений этих методов является то, что словарный запас может стать очень большим. Это, в свою очередь, потребует больших векторов для кодирования документов и налагает большие требования к памяти и замедляет алгоритмы.\n",
    "\n",
    "Умный обходной путь - использовать односторонний хэш слов, чтобы преобразовать их в целые числа. Умная часть заключается в том, что словарь не требуется, и вы можете выбрать произвольный длинный вектор фиксированной длины. Недостатком является то, что хеш является односторонней функцией, поэтому нет способа преобразовать кодировку обратно в слово (что может не иметь значения для многих контролируемых задач обучения).HashingVectorizer класс реализует этот подход, который можно использовать для последовательного хеширования слов, а затем для токенизации и кодирования документов по мере необходимости.\n",
    "\n",
    "Пример ниже демонстрирует HashingVectorizer для кодирования одного документа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "HashingVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16)\n",
      "[[ 0.37796447  0.          0.          0.          0.          0.\n",
      "   0.         -0.37796447  0.          0.         -0.37796447  0.\n",
      "   0.          0.          0.75592895  0.        ]\n",
      " [ 0.          0.          0.         -0.57735027  0.          0.\n",
      "   0.          0.          0.          0.         -0.57735027  0.\n",
      "   0.          0.          0.57735027  0.        ]\n",
      " [ 0.          0.4472136   0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.89442719  0.        ]\n",
      " [ 0.          0.          0.          0.         -0.5         0.\n",
      "   0.          0.          0.         -0.5         0.          0.\n",
      "   0.          0.5         0.5         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "document = [\"I like this movie, it's funny.\", 'I hate this movie.', 'This was awesome! I like it.', 'Nice one. I love it.']\n",
    "vectorizer = HashingVectorizer(n_features=2**4,)\n",
    "values = vectorizer.fit_transform(document)\n",
    "print(values.shape)\n",
    "print(values.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполнение примера кодирует образец документа как разреженный массив из 16 элементов. Значения закодированного документа соответствуют нормализованному количеству слов по умолчанию в диапазоне от -1 до 1, но могут быть сделаны простые целочисленные счетчики путем изменения конфигурации по умолчанию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Для чего нужны Vectorizers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритмы машинного обучения не могут напрямую работать с сырым текстом, поэтому необходимо конвертировать текст в наборы цифр (векторы). Уже с векторным представлением можно производить классификацию, к примеру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>__label__2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text       label\n",
       "0  Stuning even for the non-gamer: This sound tra...  __label__2\n",
       "1  The best soundtrack ever to anything.: I'm rea...  __label__2\n",
       "2  Amazing!: This soundtrack is my favorite music...  __label__2\n",
       "3  Excellent Soundtrack: I truly like this soundt...  __label__2\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...  __label__2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружаем данные\n",
    "data = open('corpus').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# создаем df\n",
    "trainDF = pd.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels\n",
    "trainDF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "classifier.fit(xtrain_count, train_y)\n",
    "predictions = classifier.predict(xvalid_count)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 31666)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8596"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__label__1    5097\n",
       "__label__2    4903\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDF['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vct = HashingVectorizer(analyzer='word', token_pattern=r'\\w{1,}', n_features=200)\n",
    "vct.fit(trainDF['text'])\n",
    "\n",
    "xtrain_hash =  vct.transform(train_x)\n",
    "xvalid_hash =  vct.transform(valid_x)\n",
    "\n",
    "classifier = linear_model.LogisticRegression()\n",
    "classifier.fit(xtrain_hash, train_y)\n",
    "predictions = classifier.predict(xvalid_hash)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7152"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 200)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_hash.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Conv1D, GRU, LSTM, Dropout\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.batch(16)\n",
    "valid_data = valid_data.batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw in train_data.take(1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(16,), dtype=string, numpy=\n",
       " array([b'Religion and profits: An excellent scholarly book. Great detail and very in depth analysis. Inevitable conclusion is the impossibility of molding an economic system out of a moral philosophy, not just Islam. That is where the problem with traditional Islamic thought arises, for most of its adherents, Islam is not just a set of moral principles but a way of life, dictating every aspect of it. Unavoidably, there was much effort by Islamists to set up a distinct system of banking and economy based on pure Islamic traditions. Prof. Kuran delves into the history of these movements, but he does more than that, he puts all this in cultural and political context. At the end, he bravely expands more on the general failure of Islamic culture and tradition to bring wealth and prosperity to the masses that follow it. Writing is very formal, not the easiest read and surley apporpriate only for the truly intersted in the general topic, but it was well worth it for this reader.',\n",
       "        b'good cd: this is a really good cd. i think that the song \" i think i\\'m paranoid \" is a funny song. and the rest of the cd is great',\n",
       "        b'Great book, bad product: The story contained in this book is excellent, but the book itself is junk. It appears that is was visually scanned from and older and more narrowly formatted print and then pasted into this print with little to no error checking. The worst result of this is that every word that was hyphenated across two lines in the original text appears broken and hyphenated even though it is now in the middle of a line (e.g. hyphen- ated). You will also see occasional misreads such as a number 1 in place of a lowercase \"L\".Find another printing.',\n",
       "        b\"Delicious and Addictive: This cereal is great because it tastes so good. You just crave it. But a word of warning: read the label. Last time my husband and I looked, it had saturated fat! All the other cereals in our cabinet did not... hmmmm. Another reviewer noticed this problem too!Also, Amazon's price of $23 is outrageous. It is between $1.99 to $2.50 in our stores.\",\n",
       "        b'Worthless: None of the gels came close to fitting in my ear - I ended up using the little rubber ring in the kit on my Jabra bluetooth to keep it from slipping out. Highly not recommended...',\n",
       "        b\"Don't Buy: This is the worst recording of Nirvana I have ever heard! The entire thing sounds horrible. DON'BUY IT!!\",\n",
       "        b\"Magnificent Accomplishment: This is one of those beautiful moments. You know Dolly has been kicking around major labels like a tennis ball. You know that women singers past 40 rarely get airplay. You know Dolly is a wonderful singer whose creative potential has not lessened. You bring home her latest CD on the smaller North Carolina-based Sugar Hill label; and you wonder if her recording career will be reduced to small independent releases. Then you put on the disc. Wow! I like Dolly's philosophy: When all else fails, make one of the best albums of your career. This album is nearly flawless. The steller bluegrass players lend an integrity and enthusiasm which keeps taking Dolly even higher. The songs of spiritual faith are transcendent and not preachy. This is a magnificent accomplishment. If you've like Dolly or bluegrass music, don't miss this marriage of the two. Unforgettable!\",\n",
       "        b'100 years of slow reading: This highly overrated book indicates the havoc that overweight talk show hosts can wreak upon the literary world. Garcia\\'s most juvenile and frustrating book is now being yammered about by book clubs the world over who suddenly think they know about South American authors.The book is overly long and repetitive; themes are repeated ad nauseum. Characters (some of whom have the same name) are one dimensional and unbelievable. By the time you reach the halfway point in the book you are praying for something different to happen. Then Garcia introduces a new character, has him act just like all the others, and die within 3 pages. It\\'s amazing how quickly he mows down his creations, as if he is as tired of them as I was.In the end there are no surprises, and you feel exhausted from the repetition. Underwhelming, and nowhere near as good as \"Love in the Time of Cholera\".Give it a miss and try \"Blindness\" instead.',\n",
       "        b'cool!!!!!!!!: the pet is preety cool its a nice sise and also if u want great game exp. buy more battlechips i cant give this toy 5 stars becuse when u die u need to shake it which is retadrded when i bought it at first i thought it would be a load of crap now i know that and if u buy it i want u to know',\n",
       "        b\"Absolute Waste of Time: My 11th grade English teacher tortured me with this morbid book and I can't say if I'll ever forgive her. Not only was this this book boring but Hawthorne unnecesary uses all sorts of complex language to convey one simple thought. The only reason that I didn't give this book 1 star was because there was some art in the book and symbolism. WAY OVERRATED.\",\n",
       "        b'Great Stuff: Second only to Dashiell Hammett, the works of Raymond Chandler are the best in American crime fiction. Great style with words, intrieging plots, fascinating characters, I enjoy everthing he has written. The Big Sleep is also a wonderful movie with Bogie playing the lead and Bacall the vamp.',\n",
       "        b\"Uncomfortable!: It was really easy to put the Squeem on, but once it was on I couldn't wait to get it off!! It wasn't too tight or anything. The problem was that once I stood up straight it bunched up in the hip/mid back area and pinched my skin! It was so painful! No matter how I positioned it, it would bunch up! Really disappointed since it was a waste of $50..\",\n",
       "        b\"One hundred spastic pages: I tried really to follow the story...the names are all the same,I kept constantly referring to the family tree to figure out who was who...it's more science fiction than believable...insominia disease of the entire village?!, Just too difficult for me to enjoy. I hated it and only got as far as 158 pages before I gave up.\",\n",
       "        b\"Hard read: Such a hard read for a non-psychology student - never to the point, ineptly organised, more 'jargon' type language than really necessary. Not worth it if you're not actually studying the subject or have advanced interest in psycho-analysis.\",\n",
       "        b\"comfy,warm.. but run big: just like the title says. Warm, comfy but run a size too big. I don't wear them out as they are just for lounging, so it's not that big of a deal for me.\",\n",
       "        b'A must for fans of The Band: This is a terrific video - especially if you are a fan of The Band.I never realized how great Richard Manual was until I saw this video.'],\n",
       "       dtype=object)>,\n",
       " <tf.Tensor: shape=(16,), dtype=int64, numpy=array([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1])>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "vocab_size = 10000\n",
    "seq_len = 100\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=seq_len)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim=200\n",
    "\n",
    "model = Sequential([\n",
    "    vectorize_layer,\n",
    "    Embedding(vocab_size, embedding_dim),\n",
    "    Conv1D(200, (3)),\n",
    "    Conv1D(200, (2)),\n",
    "    GRU(300),\n",
    "    #GlobalAveragePooling1D(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 83s 166ms/step - loss: 0.6949 - accuracy: 0.5065 - val_loss: 0.6927 - val_accuracy: 0.5196\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 80s 170ms/step - loss: 0.6775 - accuracy: 0.5277 - val_loss: 0.6875 - val_accuracy: 0.5200\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 81s 172ms/step - loss: 0.5190 - accuracy: 0.7053 - val_loss: 0.4106 - val_accuracy: 0.7968\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 80s 171ms/step - loss: 0.2499 - accuracy: 0.8981 - val_loss: 0.4921 - val_accuracy: 0.8104\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 78s 166ms/step - loss: 0.1206 - accuracy: 0.9560 - val_loss: 0.5988 - val_accuracy: 0.7956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f425571a3a0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, validation_data=valid_data, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = list(valid_data.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"Continuing enjoyment through the years: This has been a favorite film since my first viewing when it was initially released. The tension of the times and the quality of acting combine to reflect Hellman's personal experience. I wanted to add it to my very select library of film, now available on DVD.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[0][0][0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.828347]], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([\"Continuing enjoyment through the years: This has been a favorite film since my first viewing when it was initially released. The tension of the times and the quality of acting combine to reflect Hellman's personal experience. I wanted to add it to my very select library of film, now available on DVD.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(myNet, self).__init__()\n",
    "        self.emb = Embedding(vocab_size, embedding_dim, name=\"embedding\")\n",
    "        self.conv1 = Conv1D(200, (3))\n",
    "        self.conv2 = Conv1D(200, (3))\n",
    "        self.gPool = GlobalAveragePooling1D()\n",
    "        self.fc1 = Dense(100, activation='relu')\n",
    "        self.fc2 = Dense(1)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        x = vectorize_layer(x)\n",
    "        x = self.emb(x)\n",
    "        x1 = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.gPool((x + x1)/2)\n",
    "        x = self.fc1(x)\n",
    "        x = self.ss(x)\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel = myNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "469/469 [==============================] - 13s 25ms/step - loss: 0.5852 - accuracy: 0.6378 - val_loss: 0.4185 - val_accuracy: 0.8160\n",
      "Epoch 2/15\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 0.2444 - accuracy: 0.9018 - val_loss: 0.4274 - val_accuracy: 0.8320\n",
      "Epoch 3/15\n",
      "469/469 [==============================] - 13s 28ms/step - loss: 0.1097 - accuracy: 0.9640 - val_loss: 0.5540 - val_accuracy: 0.7940\n",
      "Epoch 4/15\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0759 - accuracy: 0.9690 - val_loss: 0.7106 - val_accuracy: 0.8244\n",
      "Epoch 5/15\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 0.0567 - accuracy: 0.9782 - val_loss: 0.7463 - val_accuracy: 0.8212\n",
      "Epoch 6/15\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 0.0328 - accuracy: 0.9883 - val_loss: 0.9834 - val_accuracy: 0.8168\n",
      "Epoch 7/15\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0224 - accuracy: 0.9909 - val_loss: 1.3219 - val_accuracy: 0.8176\n",
      "Epoch 8/15\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0226 - accuracy: 0.9910 - val_loss: 1.6931 - val_accuracy: 0.8136\n",
      "Epoch 9/15\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 0.0195 - accuracy: 0.9927 - val_loss: 1.3695 - val_accuracy: 0.8124\n",
      "Epoch 10/15\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.0185 - accuracy: 0.9919 - val_loss: 1.5048 - val_accuracy: 0.8088\n",
      "Epoch 11/15\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 0.0131 - accuracy: 0.9947 - val_loss: 1.6104 - val_accuracy: 0.8144\n",
      "Epoch 12/15\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 0.0218 - accuracy: 0.9932 - val_loss: 1.8515 - val_accuracy: 0.8116\n",
      "Epoch 13/15\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 0.0152 - accuracy: 0.9943 - val_loss: 1.0096 - val_accuracy: 0.7672\n",
      "Epoch 14/15\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 0.0152 - accuracy: 0.9925 - val_loss: 1.7764 - val_accuracy: 0.8172\n",
      "Epoch 15/15\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 0.0090 - accuracy: 0.9969 - val_loss: 1.5168 - val_accuracy: 0.8088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5cbc4745b0>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmodel.fit(train_data, validation_data=valid_data, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
