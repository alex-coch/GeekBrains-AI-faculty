# Methods-of-collecting-and-processing-data-from-the-Internet

## Lesson 01
### Основы клиент-серверного взаимодействия. Парсинг API
1. Посмотреть документацию к API GitHub, разобраться как вывести список репозиториев для конкретного пользователя, сохранить JSON-вывод в файле *.json.
2. Изучить список открытых API. Найти среди них любое, требующее авторизацию (любого типа). Выполнить запросы к нему, пройдя авторизацию через curl, Postman, Python.Ответ сервера записать в файл (приложить скриншот для Postman и curl)

## Lesson 02
### Парсинг HTML. BeautifulSoup, MongoDB
Необходимо собрать информацию о вакансиях на вводимую должность (используем input или через аргументы) с сайта superjob.ru и hh.ru. Приложение должно анализировать несколько страниц сайта(также вводим через input или аргументы). Получившийся список должен содержать в себе минимум:

- Наименование вакансии
- Предлагаемую зарплату (отдельно мин. и и отдельно макс.)
- Ссылку на саму вакансию
- Сайт откуда собрана вакансия

## Lesson 03
### Парсинг HTML. BS, SQLAlchemy
1. Развернуть у себя на компьютере/виртуальной машине/хостинге MongoDB и реализовать функцию, записывающую собранные вакансии в созданную БД
2. Написать функцию, которая производит поиск и выводит на экран вакансии с заработной платой больше введенной суммы
3. \* Написать функцию, которая будет добавлять в вашу базу данных только новые вакансии с сайта

## Lesson 04
### Парсинг HTML. XPath
Написать приложение, которое собирает основные новости с сайтов mail.ru, lenta.ru.
Для парсинга использовать xpath. Структура данных должна содержать:
- название источника,
- наименование новости,
- ссылку на новость,
- дата публикации

## Lesson 05
### Scrapy
1. Доработать паука в имеющемся проекте, чтобы он формировал item по структуре:
- Наименование вакансии
- Зарплата от
- Зарплата до
- Ссылку на саму вакансию
- Сайт откуда собрана вакансия  

И складывал все записи в БД(любую)

2. Создать в имеющемся проекте второго паука по сбору вакансий с сайта superjob. Паука должен формировать item'ы по аналогичной структуре и складывать данные также в БД
3. \* Измерить скорость сбора вакансий в проекте и сравнить ее с проектом, выполненном с использованием BS+requests

##  Lesson 06 
### Scrapy. Парсинг фото и файлов
Взять авито Авто. Собирать с использованием ItemLoader следующие данные:

- Название
- Все фото
- параметры Авто

С использованием output_processor и input_processor реализовать очистку и преобразование данных. Значения цен должны быть в виде числового значения.

Дополнительно:
Перевести всех пауков сбора данных о вакансиях на ItemLoader и привести к единой структуре.

##  Lesson 07
### Selenium
1. Написать программу, которая собирает входящие письма из своего или тестового почтового ящика и сложить данные о письмах в базу данных (от кого, дата отправки, тема письма, текст письма полный)
2. Написать программу, которая собирает «Хиты продаж» с сайта техники mvideo и складывает данные в БД. Магазины можно выбрать свои. Главный критерий выбора: динамически загружаемые товары
